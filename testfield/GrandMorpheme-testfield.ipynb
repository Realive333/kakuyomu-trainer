{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df28c50c-43f9-4854-aa1f-7db82d3e6d2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'workFactory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mMeCab\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mworkFactory\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'workFactory'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import MeCab\n",
    "import workFactory\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3bb8e09-4e86-4474-8be2-e7d253d9c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = f'./kakuyomu-data/'\n",
    "SAVE_DIR = f'./morpheme/grand/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c5a4c16-7f73-4eaa-b31b-07be21b8ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117735405488A-1.jsonl read lines: 9585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9585/9585 [00:06<00:00, 1542.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117735405488B-1.jsonl read lines: 6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6480/6480 [00:04<00:00, 1484.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117735405488C-1.jsonl read lines: 8156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8156/8156 [00:06<00:00, 1188.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117735405489A-1.jsonl read lines: 7418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7418/7418 [00:08<00:00, 880.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117735405489B-1.jsonl read lines: 6471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6471/6471 [00:04<00:00, 1311.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11773540549-1.jsonl read lines: 7539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7539/7539 [00:05<00:00, 1324.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1177354055-1.jsonl read lines: 2958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2958/2958 [00:05<00:00, 539.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681641041-1.jsonl read lines: 466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 466/466 [00:00<00:00, 1457.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681645221-1.jsonl read lines: 6774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6774/6774 [00:04<00:00, 1372.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681645222-1.jsonl read lines: 3886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3886/3886 [00:02<00:00, 1405.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681670018-1.jsonl read lines: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 1336.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681670042-1.jsonl read lines: 9254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9254/9254 [00:04<00:00, 2070.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681692761-1.jsonl read lines: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 1751.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681692785-1.jsonl read lines: 1788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1788/1788 [00:00<00:00, 2358.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681692786-1.jsonl read lines: 171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 171/171 [00:00<00:00, 2721.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4852201425-1.jsonl read lines: 1087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1087/1087 [00:01<00:00, 1007.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 154.71982288360596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "works = workFactory.read_cleaned_works(f\"{DATA_DIR}\", 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaac8336-2c7a-4f42-a752-6d16486824d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36d51f84-3cce-4e9d-b89b-a5d7a4713b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "works = works[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c75fd3b-c344-4630-a9eb-356caa3131d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1177354054880215079', 'title': 'その平面を力に変えて～かつて『ムネナシ』と呼ばれた少女は己が薄胸により最強の力を得る～', 'content': '「どうして……どうしてこうなるのですか？」若き助手は困惑していた。「どうして、だと？聡明な君らしくもない。資料の通りだ。『』１本辺りの接触範囲を最大限にし、以て効果を高める。理に適っておろう？」「確かに、そうかもしれませんが……これでは……これでは、大きければ大きいほど、効果が高まる――ということに、なりませんか？」教授の示した理論を理解するにつれ、込み上げてくる感情があった。「結果的にそうなるが、何か問題があるのかね？先ずは現状の限定された効果対象にとって、最も効果を上げることを考えておるのだ。大きさと効果が比例することに、特に不自然さはなかろう？」その答えが、助手の中の譲れない何かを刺激し、淀み渦巻いていた感情が遂に決壊した。「『』の応用理論を、このような大きさ至上主義的な内容とするべきではありませんっ！」助手は資料を作業机に叩き付け、勢いのまま強く異を唱える。「……ふむ、我輩の理論を否定するのであれば、君の理論を示したまえ。そこまで言うからには、既に君にも何かしら構想があるのだろう？」助手の剣幕に若干の当惑を示しながらも、教授は冷静に促す。理論には理論で対抗せよ。一人の研究者として、助手を扱えばこその提', 'labels': ['40'], 'wakati': ''}\n"
     ]
    }
   ],
   "source": [
    "test_w = works[100]\n",
    "print(test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0dd1b59-ef07-4fb6-a76f-3a4cabb69e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['「どうして……どうしてこうなるのですか', '', '若き助手は困惑していた', '「どうして、だと', '聡明な君らしくもない', '資料の通りだ', '『』１本辺りの接触範囲を最大限にし、以て効果を高める', '理に適っておろう', '', '「確かに、そうかもしれませんが……これでは……これでは、大きければ大きいほど、効果が高まる――ということに、なりませんか', '', '教授の示した理論を理解するにつれ、込み上げてくる感情があった', '「結果的にそうなるが、何か問題があるのかね', '先ずは現状の限定された効果対象にとって、最も効果を上げることを考えておるのだ', '大きさと効果が比例することに、特に不自然さはなかろう', '', 'その答えが、助手の中の譲れない何かを刺激し、淀み渦巻いていた感情が遂に決壊した', '「『』の応用理論を、このような大きさ至上主義的な内容とするべきではありませんっ', '', '助手は資料を作業机に叩き付け、勢いのまま強く異を唱える', '「……ふむ、我輩の理論を否定するのであれば、君の理論を示したまえ', 'そこまで言うからには、既に君にも何かしら構想があるのだろう', '', '助手の剣幕に若干の当惑を示しながらも、教授は冷静に促す', '理論には理論で対抗せよ', '一人の研究者として、助手を扱えばこその提']\n",
      "['助手', '困惑', '聡明', '資料', '通り', '１', '辺り', '接触', '範囲', '最大', '効果', '理', '効果', 'こと', '教授', '理論', '理解', '感情', '結果', '問題', '現状', '限定', '効果', '対象', '効果', 'こと', '効果', '比例', 'こと', '答え', '助手', '中', '刺激', '淀み', '感情', '決壊', '応用', '理論', '至上', '主義', '内容', '助手', '資料', '作業', '机', '勢い', 'まま', '異', '理論', '否定', '理論', 'まえ', '構想', '助手', '剣幕', '若干', '当惑', '教授', '冷静', '理論', '理論', '対抗', '一人', '研究', '助手', '提']\n",
      "[' 「 どう し て … … どう し て こう なる の です か ', ' ', ' 若き 助手 は 困惑 し て い た ', ' 「 どう し て 、 だ と ', ' 聡明 な 君 らしく も ない ', ' 資料 の 通り だ ', ' 『 』 １ 本 辺り の 接触 範囲 を 最大 限 に し 、 以て 効果 を 高める ', ' 理 に 適っ て おろう ', ' ', ' 「 確か に 、 そう か も しれ ませ ん が … … これ で は … … これ で は 、 大きけれ ば 大きい ほど 、 効果 が 高まる ―― と いう こと に 、 なり ませ ん か ', ' ', ' 教授 の 示し た 理論 を 理解 する に つれ 、 込み上げ て くる 感情 が あっ た ', ' 「 結果 的 に そう なる が 、 何 か 問題 が ある の か ね ', ' 先ず は 現状 の 限定 さ れ た 効果 対象 に とっ て 、 最も 効果 を 上げる こと を 考え て おる の だ ', ' 大き さ と 効果 が 比例 する こと に 、 特に 不 自然 さ は なかろう ', ' ', ' その 答え が 、 助手 の 中 の 譲れ ない 何 か を 刺激 し 、 淀み 渦巻い て い た 感情 が 遂に 決壊 し た ', ' 「 『 』 の 応用 理論 を 、 この よう な 大き さ 至上 主義 的 な 内容 と する べき で は あり ませ ん っ ', ' ', ' 助手 は 資料 を 作業 机 に 叩き付け 、 勢い の まま 強く 異 を 唱える ', ' 「 … … ふむ 、 我輩 の 理論 を 否定 する の で あれ ば 、 君 の 理論 を 示し た まえ ', ' そこ まで 言う から に は 、 既に 君 に も 何 かしら 構想 が ある の だろう ', ' ', ' 助手 の 剣幕 に 若干 の 当惑 を 示し ながら も 、 教授 は 冷静 に 促す ', ' 理論 に は 理論 で 対抗 せよ ', ' 一人 の 研究 者 と し て 、 助手 を 扱え ば こそ の 提 ']\n"
     ]
    }
   ],
   "source": [
    "cnt = test_w['content']\n",
    "sep = re.split(\"。|」|？|！\",cnt)\n",
    "print(sep)\n",
    "mecab = MeCab.Tagger(\"\")\n",
    "\n",
    "s_list = []\n",
    "noun_list = []\n",
    "verb_list = []\n",
    "adjv_list = []\n",
    "\n",
    "\n",
    "for sent in sep:\n",
    "    node = mecab.parseToNode(sent)\n",
    "    surface = []\n",
    "    while node:\n",
    "        surface.append(node.surface)\n",
    "        term = node.feature.split(\",\")[0]\n",
    "        if term == \"名詞\":\n",
    "            noun_list.append(node.surface)\n",
    "        elif term == \"動詞\":\n",
    "            verb_list.append(node.surface)\n",
    "        elif term == \"形容詞\" or term == \"副詞\":\n",
    "            adjv_list.append(node.surface)\n",
    "        else:\n",
    "            pass\n",
    "        node = node.next\n",
    "    s_list.append(' '.join(surface))\n",
    "\n",
    "print(noun_list)\n",
    "print(s_list)\n",
    "with open(f'./morpheme/grand/test', 'w', encoding='utf-8') as f:\n",
    "        for line in s_list:\n",
    "            f.write(f'{line}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac479314-9dc6-4180-85aa-8304d0be803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(n, v, a, path, work):\n",
    "    noun_list = []\n",
    "    verb_list = []\n",
    "    adjv_list = []\n",
    "    surface_list =[]\n",
    "    \n",
    "    cnt = work['content']\n",
    "    sep = re.split(\"。|」|？|！\", cnt)\n",
    "    \n",
    "    mecab = MeCab.Tagger(\"\")\n",
    "    for sent in sep:\n",
    "        node = mecab.parseToNode(sent)\n",
    "        surface = []\n",
    "        while node:\n",
    "            surface.append(node.surface)\n",
    "            term = node.feature.split(\",\")[0]\n",
    "            if term == \"名詞\":\n",
    "                noun_list.append(node.surface)\n",
    "            elif term == \"動詞\":\n",
    "                verb_list.append(node.surface)\n",
    "            elif term == \"形容詞\" or term == \"副詞\":\n",
    "                adjv_list.append(node.surface)\n",
    "            else:\n",
    "                pass\n",
    "            node = node.next\n",
    "        surface_list.append(' '.join(surface))\n",
    "    \n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(f'{path}/{work[\"id\"]}', 'w', encoding='utf-8') as f:\n",
    "        for line in surface_list:\n",
    "            f.write(f'{line}\\n')\n",
    "            \n",
    "    for noun in noun_list:\n",
    "        n[noun] += 1\n",
    "            \n",
    "    for verb in verb_list:\n",
    "        v[verb] += 1\n",
    "            \n",
    "    for adjv in adjv_list:\n",
    "        a[adjv] += 1\n",
    "    \n",
    "    return n, v, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b37d30e5-1ff0-42f1-8664-3b0d59067da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 93.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 94.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 94.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 94.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 94.46it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for work in batch(works, 200):\n",
    "    print(f'batch={i}')\n",
    "    \n",
    "    noun = defaultdict(int)\n",
    "    verb = defaultdict(int)\n",
    "    adjv = defaultdict(int)\n",
    "    \n",
    "    for w in tqdm(work):\n",
    "        noun, verb, adjv = analysis(noun, verb, adjv, f'{SAVE_DIR}/{i}', w)\n",
    "    \n",
    "    noun = OrderedDict(sorted(noun.items(), key=lambda x:x[1], reverse=True))\n",
    "    verb = OrderedDict(sorted(verb.items(), key=lambda x:x[1], reverse=True))\n",
    "    adjv = OrderedDict(sorted(adjv.items(), key=lambda x:x[1], reverse=True))\n",
    "    \n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    with open(f'{SAVE_DIR}/{i}/noun.tsv', 'w') as f:\n",
    "        w = csv.writer(f, delimiter='\\t')\n",
    "        w.writerow(['word', 'count'])\n",
    "        for key, val in noun.items():\n",
    "            w.writerow([key, val])\n",
    "    \n",
    "    with open(f'{SAVE_DIR}/{i}/verb.tsv', 'w') as f:\n",
    "        w = csv.writer(f, delimiter='\\t')\n",
    "        w.writerow(['word', 'count'])\n",
    "        for key, val in verb.items():\n",
    "            w.writerow([key, val])\n",
    "            \n",
    "    with open(f'{SAVE_DIR}/{i}/adjv.tsv', 'w') as f:\n",
    "        w = csv.writer(f, delimiter='\\t')\n",
    "        w.writerow(['word', 'count'])\n",
    "        for key, val in adjv.items():\n",
    "            w.writerow([key, val])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc17efbe-59b6-4698-af5c-7a12dfb80e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ラベル と 形態 素 分析 し た 名詞 の リスト を それぞれ Word 2 Vec で 変換 する \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import MeCab\n",
    "txt = '妖力 と 妖術 と いう 二 つ の 能力 が ある の 妖力 と 妖術 凱 斗 に は 正直 どちら も 同じ に 聞こえ た\n",
    "一体 何 が どう 違う と いう の か 妖術 は 呪文 みたい な もの ね 手順 を 踏む こと で 発動 する もの を\n",
    "指す の 一方 の 妖力 、 これ は 常時 発動 し て いる 妖魅 特有 の 力 の こと よ 例えば >、 と 言っ て 穂積\n",
    "は 人差し 指 を 立て た 幽霊 、 って いう の は 身体 的 特徴 と し て 『 実態 が ない 』 し 『 半 透明 』\n",
    "だっ たり する わ よ ね 一 >反 木綿 だっ たら 『 飛行 』 できる と か こう いっ た 常態 的 に 発揮 さ れ \n",
    "て いる 能力 を \"\" 妖力 \"\" と 呼ぶ の 立て て い た 指 で 赤い 縁 の 眼鏡 を 押し上げ 穂積 は 続ける それ\n",
    "と は 別 に 、 狸 が 『 化ける 』 だ と か 、 セイ レーン が その 歌声 で 『 魅了 』 する だ と か 、 雷神\n",
    "が 『 雷撃 』 を 放っ たり と か 能力 を 発動 する ため に プロセス や 集中 など の 作業 が 必要 な もの \n",
    "を 総称 し て \"\" 妖術 \"\" と 呼ん で いる の さっき 貴方 が 出し た 火柱 も 妖術 に 分類 さ れる わ 凱 斗 \n",
    "は 自分 の 手 を 見る 指先 から 燃え上がっ た 炎 の 柱 の こと を 思い出す 確か に 燃えろ と 念じ た 途端 \n",
    "に 起>こっ た 出来事 だっ た 貴方 の 場合 、 古 籠 火 の 骸 露 を 取り入れ た こと で 火 に 関する 妖術 を\n",
    "扱える よう に なっ て いる って わけ 炎 の 妖術 は 推し 並べ て 強力 な 物 が 多い から 、 ここ で 扱い \n",
    "方 を 学ん で おく と 後々 役 に 立つ わ 断定 し 、 さあ やっ て み て と ばかり に 穂積 は 顎 を\"\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "mecab.parse('ラベルと形態素分析した名詞のリストをそれぞれWord2Vecで変換する')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64237c84-bc4c-40be-b17b-1f2c49e6a244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
